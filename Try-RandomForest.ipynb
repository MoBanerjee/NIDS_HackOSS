{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5a034e-25d7-4e28-87d8-8aa712370e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iker\\AppData\\Local\\Temp\\ipykernel_29508\\1756040219.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943b31d3-455c-4dbd-9d9c-9b268c8e48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_csv(\"TotalDataset_Engineered.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2efcdfd-cf2d-4dcf-8cd1-69d5cfc69495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date first seen</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Proto</th>\n",
       "      <th>Src IP Addr</th>\n",
       "      <th>Src Pt</th>\n",
       "      <th>Dst IP Addr</th>\n",
       "      <th>Dst Pt</th>\n",
       "      <th>Packets</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>...</th>\n",
       "      <th>S</th>\n",
       "      <th>F</th>\n",
       "      <th>class</th>\n",
       "      <th>attackType</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>days</th>\n",
       "      <th>hours</th>\n",
       "      <th>minutes</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39643004</th>\n",
       "      <td>39643004</td>\n",
       "      <td>2019-03-06 21:18:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UDP</td>\n",
       "      <td>216.239.36.10</td>\n",
       "      <td>53</td>\n",
       "      <td>158.129.192.2</td>\n",
       "      <td>48598.0</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>none</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643005</th>\n",
       "      <td>39643005</td>\n",
       "      <td>2019-03-06 21:18:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UDP</td>\n",
       "      <td>194.0.4.10</td>\n",
       "      <td>53</td>\n",
       "      <td>192.71.30.8</td>\n",
       "      <td>34617.0</td>\n",
       "      <td>2</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>none</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643006</th>\n",
       "      <td>39643006</td>\n",
       "      <td>2019-03-06 21:18:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UDP</td>\n",
       "      <td>83.171.8.8</td>\n",
       "      <td>53</td>\n",
       "      <td>31.13.112.156</td>\n",
       "      <td>19981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>none</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643007</th>\n",
       "      <td>39643007</td>\n",
       "      <td>2019-03-06 21:18:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UDP</td>\n",
       "      <td>192.71.30.8</td>\n",
       "      <td>34617</td>\n",
       "      <td>194.0.4.10</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>none</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643008</th>\n",
       "      <td>39643008</td>\n",
       "      <td>2019-03-06 21:18:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UDP</td>\n",
       "      <td>2620:0:cc9::70</td>\n",
       "      <td>27547</td>\n",
       "      <td>2001:678:6::1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>none</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0      Date first seen  Duration Proto     Src IP Addr  \\\n",
       "39643004    39643004  2019-03-06 21:18:57       0.0   UDP   216.239.36.10   \n",
       "39643005    39643005  2019-03-06 21:18:57       0.0   UDP      194.0.4.10   \n",
       "39643006    39643006  2019-03-06 21:18:57       0.0   UDP      83.171.8.8   \n",
       "39643007    39643007  2019-03-06 21:18:57       0.0   UDP     192.71.30.8   \n",
       "39643008    39643008  2019-03-06 21:18:57       0.0   UDP  2620:0:cc9::70   \n",
       "\n",
       "          Src Pt    Dst IP Addr   Dst Pt  Packets  Bytes  ...  S  F   class  \\\n",
       "39643004      53  158.129.192.2  48598.0        2    290  ...  0  0  normal   \n",
       "39643005      53    192.71.30.8  34617.0        2    268  ...  0  0  normal   \n",
       "39643006      53  31.13.112.156  19981.0        2    136  ...  0  0  normal   \n",
       "39643007   34617     194.0.4.10     53.0        2    138  ...  0  0  normal   \n",
       "39643008   27547  2001:678:6::1     53.0        2    174  ...  0  0  normal   \n",
       "\n",
       "          attackType  year  month days hours  minutes  seconds  \n",
       "39643004        none  2019      3    6    21       18     57.0  \n",
       "39643005        none  2019      3    6    21       18     57.0  \n",
       "39643006        none  2019      3    6    21       18     57.0  \n",
       "39643007        none  2019      3    6    21       18     57.0  \n",
       "39643008        none  2019      3    6    21       18     57.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ce48d-c38c-47b1-a56b-61d129d08f30",
   "metadata": {},
   "source": [
    "Change variables types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958164bb-8442-43c6-8abf-9b04d8335628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39643009 entries, 0 to 39643008\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   Unnamed: 0       int64  \n",
      " 1   Date first seen  object \n",
      " 2   Duration         float64\n",
      " 3   Proto            object \n",
      " 4   Src IP Addr      object \n",
      " 5   Src Pt           int64  \n",
      " 6   Dst IP Addr      object \n",
      " 7   Dst Pt           float64\n",
      " 8   Packets          int64  \n",
      " 9   Bytes            int64  \n",
      " 10  U                int64  \n",
      " 11  A                int64  \n",
      " 12  P                int64  \n",
      " 13  R                int64  \n",
      " 14  S                int64  \n",
      " 15  F                int64  \n",
      " 16  class            object \n",
      " 17  attackType       object \n",
      " 18  year             int64  \n",
      " 19  month            int64  \n",
      " 20  days             int64  \n",
      " 21  hours            int64  \n",
      " 22  minutes          int64  \n",
      " 23  seconds          float64\n",
      "dtypes: float64(3), int64(15), object(6)\n",
      "memory usage: 7.1+ GB\n"
     ]
    }
   ],
   "source": [
    "total_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2224c60e-397a-4a3a-b44b-5359185607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['year'] = total_data['year'].astype('int64')\n",
    "total_data['month'] = total_data['month'].astype('int64')\n",
    "total_data['days'] = total_data['days'].astype('int64')\n",
    "total_data['hours'] = total_data['hours'].astype('int64')\n",
    "total_data['minutes'] = total_data['minutes'].astype('int64')\n",
    "total_data['seconds'] = total_data['seconds'].astype('int64')\n",
    "total_data['Dst Pt'] = total_data['Dst Pt'].astype('int64')\n",
    "total_data['Proto'] = total_data['Proto'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bb5d43-c996-4295-aea1-d5ccb25effdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39643009 entries, 0 to 39643008\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Dtype   \n",
      "---  ------           -----   \n",
      " 0   Unnamed: 0       int64   \n",
      " 1   Date first seen  object  \n",
      " 2   Duration         float64 \n",
      " 3   Proto            category\n",
      " 4   Src IP Addr      object  \n",
      " 5   Src Pt           int64   \n",
      " 6   Dst IP Addr      object  \n",
      " 7   Dst Pt           int64   \n",
      " 8   Packets          int64   \n",
      " 9   Bytes            int64   \n",
      " 10  U                int64   \n",
      " 11  A                int64   \n",
      " 12  P                int64   \n",
      " 13  R                int64   \n",
      " 14  S                int64   \n",
      " 15  F                int64   \n",
      " 16  class            object  \n",
      " 17  attackType       object  \n",
      " 18  year             int64   \n",
      " 19  month            int64   \n",
      " 20  days             int64   \n",
      " 21  hours            int64   \n",
      " 22  minutes          int64   \n",
      " 23  seconds          int64   \n",
      "dtypes: category(1), float64(1), int64(17), object(5)\n",
      "memory usage: 6.8+ GB\n"
     ]
    }
   ],
   "source": [
    "total_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c6399-971a-4fd5-bd55-d019585c6e21",
   "metadata": {},
   "source": [
    "Categorizing the Proto as Mark did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04be0fc9-fc25-4d7a-8543-81e3c118d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_enc_df = pd.get_dummies(total_data['Proto'], dtype='int64')\n",
    "\n",
    "total_data = .join(proto_enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4eb28dda-5e17-4e40-8421-341e195ccfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['class'] = total_data['class'].map(\n",
    "    {\"normal\":0,\"victim\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af53621c-3977-4a0d-a191-ea65404b1a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d2fee-ebfb-487c-8be9-dadf1bc1c95c",
   "metadata": {},
   "source": [
    "Standarization is not needed for Random Forest application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "766e8462-1050-443a-b73e-e7fba4d9f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Duration', 'Src Pt', 'Dst Pt', 'Packets', 'Bytes', 'U', 'A', 'P', 'R', 'S', 'F', 'year', 'month', 'days', 'hours', 'minutes', 'seconds', 'AH', 'ESP', 'GRE', 'ICMP', 'ICMP6', 'IGMP', 'IPIP', 'IPv6', 'OSPF', 'PIM', 'TCP', 'UDP', 'VRRP']\n"
     ]
    }
   ],
   "source": [
    "features = list(total_data.columns)\n",
    "features.remove(\"Unnamed: 0\")\n",
    "features.remove(\"Date first seen\")\n",
    "features.remove('Src IP Addr')\n",
    "features.remove('Dst IP Addr')\n",
    "features.remove('Proto')\n",
    "features.remove(\"class\")\n",
    "features.remove(\"attackType\")\n",
    "\n",
    "\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe3b95-b856-41b0-a4e8-a6bd7fcb2b74",
   "metadata": {},
   "source": [
    "Let's create the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b191946-01e2-4dc7-b5d3-86e0ce300655",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pruebas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      2\u001b[0m pruebas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnomaly\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pruebas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpruebas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "total_data['class'].value_counts()\n",
    "total_data['Anomaly'] = total_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4004e977-8234-45f2-b751-1689de23414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = total_data.drop('class',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54177ad7-b3fc-44ec-846d-381bee754609",
   "metadata": {},
   "source": [
    "Using the same splitting class as MARK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f80b23ba-3525-4d40-b907-62c824fb7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split():\n",
    "    \"\"\"\n",
    "    Mark's Split class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num = 5):  # num refers to the number of datasets you wanna split the original total dataset into\n",
    "        self.total_data = total_data\n",
    "        self._0 = []\n",
    "        self._1 = []\n",
    "        self.cv = num\n",
    "\n",
    "    def generate(self):\n",
    "        self._split()\n",
    "\n",
    "        res = []\n",
    "        \n",
    "        for i in tqdm(range(self.cv)):\n",
    "            if i != self.cv-1:\n",
    "                tmp = self._0[i*(len(self._0)//self.cv):(i+1)*(len(self._0)//self.cv)] + self._1[i*(len(self._1)//self.cv):(i+1)*(len(self._1)//self.cv)]\n",
    "            elif i == self.cv-1:\n",
    "                tmp = self._0[i*(len(self._0)//self.cv):-1] + self._1[i*(len(self._1)//self.cv):-1]\n",
    "            res.append(tmp)\n",
    "\n",
    "        return res # Returns arrays of indices\n",
    "        \n",
    "    def _split(self):\n",
    "        length = len(self.total_data)\n",
    "        print(\"Splitting...\")\n",
    "        for i in tqdm(range(length)):\n",
    "            ttype = self.total_data.iloc[i].Anomaly\n",
    "            if ttype == 0:\n",
    "                    self._0.append(i)\n",
    "            else:\n",
    "                    self._1.append(i)\n",
    "\n",
    "        print(\"Splitting done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2acda1e1-857c-478d-adba-24c6dfbe7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1718.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 3166.95it/s]\n"
     ]
    }
   ],
   "source": [
    "split = Split(num = n_subset) # num refers to the number of datasets you wanna split the original total dataset into\n",
    "split_data = split.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "461b12d0-9656-4212-883f-0ff3c279fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = split_data[0] + split_data[1] + split_data[2] + split_data[3]\n",
    "test_validation_data = split_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e7e868b1-d4e7-486e-9578-48c5fb75e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1\n",
      "----------------------\n",
      "200\n",
      "\n",
      "Subset - 1 | Fold - 1\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 50, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 2\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 100, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 3\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 200, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 1s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 4\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 50, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 5\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 100, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 6\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 200, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 1s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 7\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 50, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 8\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 100, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 1 | Fold - 9\n",
      "----------------------\n",
      "Training...\n",
      "Using model with parameters :{'n_estimators': 200, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "------+++++++--------\n",
      "------+++++++--------\n",
      "                                                 Average accuracy: 0.00%\n",
      "------+++++++--------\n",
      "------+++++++--------\n",
      "Subset 2\n",
      "----------------------\n",
      "200\n",
      "\n",
      "Subset - 2 | Fold - 1\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 50, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 2\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 100, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 3\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 200, 'max_depth': None}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 4\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 50, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 5\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 100, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 6\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 200, 'max_depth': 10}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 1s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 7\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 50, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 8\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 100, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 9\n",
      "----------------------\n",
      "Training using saved model  :{'n_estimators': 200, 'max_depth': 20}\n",
      "----------------------\n",
      "Training done!\n",
      "----------------------\n",
      "Time taken: 0s\n",
      "----------------------\n",
      "\n",
      "Subset - 2 | Fold - 10\n",
      "----------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "ParameterGrid index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining using saved model  :\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mparam_combinations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Batch Training\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:211\u001b[0m, in \u001b[0;36mParameterGrid.__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    208\u001b[0m             out[key] \u001b[38;5;241m=\u001b[39m v_list[offset]\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameterGrid index out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: ParameterGrid index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n",
    "\n",
    "# Define hyperparameters list for RandomForest\n",
    "hyperparameters = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = ParameterGrid(hyperparameters)\n",
    "\n",
    "# Training various different models for reliability\n",
    "models = [RandomForestClassifier(**params, random_state = 0) for params in param_combinations]\n",
    "\n",
    "# Batch Training our model\n",
    "testing_subset_number = n_subset-1\n",
    "total_subset_arr = [i for i in range(n_subset)]\n",
    "total_subset_arr.remove(testing_subset_number) #removing last subset\n",
    "\n",
    "acc = np.zeros((len(models),n_subset))\n",
    "\n",
    "for subset in total_subset_arr:  # Testing on the last subset\n",
    "    print(\"Subset {}\".format(subset + 1))\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Obtaining our i-th subset of the total data\n",
    "    X = total_data.iloc[split_data[subset]][features]\n",
    "    y = total_data.iloc[split_data[subset]]['Anomaly']\n",
    "\n",
    "    # Applying StratifiedKFold to the specific subset\n",
    "    skf = StratifiedKFold(n_splits=len(models))\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nSubset - {subset + 1} | Fold - {i + 1}\")\n",
    "        print(\"----------------------\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Paste models here\n",
    "        model = models[i]\n",
    "\n",
    "        # Train\n",
    "        if subset == 0:\n",
    "            print(\"Training...\")\n",
    "            print(\"Using model with parameters :\"+str(param_combinations[i]))\n",
    "            print(\"----------------------\")\n",
    "            model.fit(X_train, y_train)\n",
    "        else:\n",
    "            print(\"Training using saved model  :\"+str(param_combinations[i]))\n",
    "            print(\"----------------------\")  # Batch Training\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Training done!\")\n",
    "        print(\"----------------------\")\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "        # Predict\n",
    "        pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        #acc.append(accuracy)\n",
    "\n",
    "        # Printing\n",
    "        #print(\"Subset - {} | Fold - {} Accuracy: {:.2f}%\".format(subset + 1, i + 1, accuracy * 100))\n",
    "        print(\"Time taken: {:.0f}s\".format(time.time() - start_time))\n",
    "        print(\"----------------------\")\n",
    "\n",
    "    print(\"------+++++++--------\")\n",
    "    print(\"------+++++++--------\")\n",
    "    print(\"                                                 Average accuracy: {:.2f}%\".format(np.mean(acc) * 100))\n",
    "    print(\"------+++++++--------\")\n",
    "    print(\"------+++++++--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bdf321c7-44eb-4436-ac12-bd8629244d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1\n",
      "----------------------\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best hyperparameters are: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 7, 'max_depth': 10}\n",
      "Best score is: 0.8800000000000001\n",
      "Subset 2\n",
      "----------------------\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best hyperparameters are: {'n_estimators': 1200, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 1, 'max_depth': 40}\n",
      "Best score is: 0.9400000000000001\n",
      "Subset 3\n",
      "----------------------\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best hyperparameters are: {'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1, 'max_depth': 40}\n",
      "Best score is: 0.9\n",
      "Subset 4\n",
      "----------------------\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best hyperparameters are: {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 7, 'max_depth': 60}\n",
      "Best score is: 0.8949999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_features':[1,3,5,7],\n",
    "              'min_samples_leaf':[2,3],\n",
    "              'min_samples_split':[2,3]\n",
    "               }\n",
    "\n",
    "testing_subset_number = n_subset-1\n",
    "total_subset_arr = [i for i in range(n_subset)]\n",
    "total_subset_arr.remove(testing_subset_number) #removing last subset\n",
    "\n",
    "list_rf = []\n",
    "\n",
    "for subset in total_subset_arr:  # Testing on the last subset\n",
    "    print(\"Subset {}\".format(subset + 1))\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Obtaining our i-th subset of the total data\n",
    "    X = total_data.iloc[split_data[subset]][features]\n",
    "    y = total_data.iloc[split_data[subset]]['Anomaly']\n",
    "\n",
    "   \n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X, y)\n",
    "\n",
    "    list_rf.append(rf_random)\n",
    "\n",
    "    print('Best hyperparameters are: '+str(rf_random.best_params_))\n",
    "    print('Best score is: '+str(rf_random.best_score_))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68379af5-529f-4772-be36-bed72438f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpp= X_train\n",
    "ypp= y_train\n",
    "\n",
    "\n",
    "rfpp = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_randompp = RandomizedSearchCV(estimator = rfpp, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_randompp.fit(Xpp, ypp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "696e868d-87b4-4b3d-b998-4edb20aeee83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'n_estimators': 2000, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 5, 'max_depth': 10}\n",
      "Best score is: 0.9087500000000001\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters are: '+str(rf_randompp.best_params_))\n",
    "print('Best score is: '+str(rf_randompp.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "50b50259-48d1-4303-ae7c-0aad7bf650f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL\n",
      "Model Performance\n",
      "Average Error: 0.0960 degrees.\n",
      "Accuracy = 0.90404%.\n",
      "_______________________\n",
      "Base Accuracy = 0.90%.\n",
      "{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 7, 'max_depth': 10}\n",
      "Model Performance\n",
      "Average Error: 0.0909 degrees.\n",
      "Accuracy = 0.90909%.\n",
      "{'n_estimators': 1200, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 1, 'max_depth': 40}\n",
      "Model Performance\n",
      "Average Error: 0.0960 degrees.\n",
      "Accuracy = 0.90404%.\n",
      "{'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1, 'max_depth': 40}\n",
      "Model Performance\n",
      "Average Error: 0.0960 degrees.\n",
      "Accuracy = 0.90404%.\n",
      "{'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 7, 'max_depth': 60}\n",
      "Model Performance\n",
      "Average Error: 0.0909 degrees.\n",
      "Accuracy = 0.90909%.\n",
      "{'n_estimators': 2000, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 5, 'max_depth': 10}\n",
      "Model Performance\n",
      "Average Error: 0.0909 degrees.\n",
      "Accuracy = 0.90909%.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.5f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "X_train = total_data.iloc[train_validation_data][features]\n",
    "y_train = total_data.iloc[train_validation_data]['Anomaly']\n",
    "X_test = total_data.iloc[test_validation_data][features]\n",
    "y_test = total_data.iloc[test_validation_data]['Anomaly']\n",
    "\n",
    "base_model = RandomForestClassifier(random_state = 0)\n",
    "base_model.fit(X_train, y_train)\n",
    "print(\"BASE MODEL\")\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "print('_______________________')\n",
    "print('Base Accuracy = {:0.2f}%.'.format(base_accuracy))\n",
    "\n",
    "model1 = RandomForestClassifier(**list_rf[0].best_params_, random_state = 0)\n",
    "print(list_rf[0].best_params_)\n",
    "model1.fit(X_train, y_train)\n",
    "model1_accuracy = evaluate(model1,X_test,y_test)\n",
    "\n",
    "model2 = RandomForestClassifier(**list_rf[1].best_params_, random_state = 0)\n",
    "print(list_rf[1].best_params_)\n",
    "model2.fit(X_train, y_train)\n",
    "model2_accuracy = evaluate(model2,X_test,y_test)\n",
    "\n",
    "model3 = RandomForestClassifier(**list_rf[2].best_params_, random_state = 0)\n",
    "print(list_rf[2].best_params_)\n",
    "model3.fit(X_train, y_train)\n",
    "model3_accuracy = evaluate(model3,X_test,y_test)\n",
    "\n",
    "model4 = RandomForestClassifier(**list_rf[3].best_params_, random_state = 0)\n",
    "print(list_rf[3].best_params_)\n",
    "model4.fit(X_train, y_train)\n",
    "model4_accuracy = evaluate(model4,X_test,y_test)\n",
    "\n",
    "model5 = RandomForestClassifier(**rf_randompp.best_params_, random_state = 0)\n",
    "print(rf_randompp.best_params_)\n",
    "model5.fit(X_train, y_train)\n",
    "model5_accuracy = evaluate(model4,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
